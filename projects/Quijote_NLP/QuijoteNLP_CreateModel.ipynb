{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto NLP Quijote\n",
    "\n",
    "**Objetivo**\n",
    "Creación de un modelo de Deep Learning entrenado con los primeros **50 capitulos del Quijote**, para crear una contextualización artificial del contenido del libro y de esta manera poder predecir un nuevo texto en función a unas palabras dadas.\n",
    "\n",
    "**Requisitos**\n",
    "* Python 3.8\n",
    "* Tensorflow 2.x\n",
    "\n",
    "**Pasos de creación**\n",
    "* Adquirir el libro del Quijote en formato digital. https://www.gutenberg.org/ebooks/search/?query=quijote&submit_search=Go%21\n",
    "* Tratamiento de los datos\n",
    "* Creación del modelo usando redes LSTM (Large Short Term Memory)\n",
    "* Entrenamiento\n",
    "* Resultados y validación\n",
    "* Prediccón\n",
    "* Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzBKcqmSO-bq"
   },
   "outputs": [],
   "source": [
    "#Cargamos las librerias correspondientes\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Como la palabra sugiere tokenizar significa dividir la oración en una serie de tokens o en palabras simples, podemos decir que cada vez que hay un espacio en una oración agregamos una coma entre ellos para que nuestra oración se divida en tokens y cada palabra tenga un valor único de un número entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "PLkTiMhlPWfq",
    "outputId": "af0789ef-4dec-4e02-e71c-f350f9cdc96a"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = open('Dataset/quijote_Lite.txt', encoding=\"utf8\").read()\n",
    "#data = open('/content/drive/My Drive/Colab Notebooks/Datasets/Vuelta_al_mundo.txt').read()\n",
    "\n",
    "#Limpiar de simbolos \n",
    "data = re.sub('[^a-zA-Z0-9á-ú\\¿\\?\\n\\.]', ' ', data)\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "#Mostramos el cuerpo del texto ya limpio\n",
    "#print(corpus)\n",
    "corpus = sorted(list(set(corpus)))\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado despues de realizar los **tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "0ZGKiyUcRbs6",
    "outputId": "8b64452d-fadb-4204-b846-6e015ff19c60"
   },
   "outputs": [],
   "source": [
    "#print(tokenizer.word_index)\n",
    "#print(total_words)\n",
    "#print(corpus)\n",
    "#print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación de los datos\n",
    "Para mejorar el entrenamiento del modelo y poder tener más datos a partir de los obtenidos, se realiza una tecnica llamda **secuencia**. Que consiste en dividir cada oración en una más pequeña en forma de escalera, de esta forma se hará una predicción de entrenamiento por cada subdivisión de esa oración.\n",
    "\n",
    "\n",
    "Posteriormente se realiza un **Padding**. Es un método para convertir la matriz de enteros de longitud variable en una longitud fija, ya sea truncando (si la longitud es mayor que la longitud_máxima definida que trunca la matriz) o rellenando (si la longitud es más corta que la longitud_máxima, rellene la matriz con 0).\n",
    "\n",
    "<img src=\"images/Padding.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhEV05uxPXrI"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "#Marcamos los tokens por cada frase\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\t#Creamos frases más pequeñas en función a al original\n",
    "\t#for i in range(1, len(token_list)):\n",
    "\tfor i in range(1, len(token_list)): #usamos cada frase para aumentar el train en modo de escalera\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "#Buscamos la frase más larga y las igualamos todas con ceros en la parte de adelante\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "#Cogemos el ultimo valor como etiqueta (y) y el resto como (x)\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mostramos algunos ejemplos del proceso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "bGqeJ-NXPeGA",
    "outputId": "2a7a8d1f-298f-48de-a9fb-bd8043eb8d3d"
   },
   "outputs": [],
   "source": [
    "print(xs[5])\n",
    "print(ys[5])\n",
    "print(ys.shape)\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo\n",
    "\n",
    "* La primera capa incluye el número de palabras a entrenar y la salida de predicción que queremos mostrar, en este caso se hará una predicción de 50 palabras en función a la dada.\n",
    "\n",
    "* Usamos como optimizador Adam, aunque tambien se optienen buenos resultados con RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "zxvxkZIEPgz3",
    "outputId": "bae380ea-7c2d-41a8-f06b-d2d1f830a4a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.001)\n",
    "rms=RMSprop(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "#filepath=\"/content/drive/My Drive/Colab Notebooks/Projects/QuijoteNLP/model/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "filepath=\"model/weights-QuijoteNLP.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar pesos del modelo (evitar entrenamiento)\n",
    "model.load_weights(\"model/weights-QuijoteNLP.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizar el consumo energético e impacto ambiental del enetrenamiento del modelo mediante **CodeCarbon**\n",
    "CodeCarbon es un proyecto de código abierto diseñado para medir la huella de carbono de los algoritmos de inteligencia artificial.\n",
    "\n",
    "#### ¿Cómo funciona?\n",
    "\n",
    "* Monitoreo Transparente: CodeCarbon funciona en segundo plano mientras entrenas tus modelos de IA. Recopila datos sobre el consumo de energía y utiliza información sobre la ubicación del servidor y los tipos de fuentes de energía para calcular la huella de carbono.\n",
    "\n",
    "* Facilidad de Uso: Integrar CodeCarbon en proyectos de IA es sencillo. Esto lo hace accesible para investigadores, desarrolladores y educadores que deseen medir el impacto ambiental de su trabajo.\n",
    "\n",
    "* Resultados Accionables: La herramienta no solo mide la huella de carbono, sino que también proporciona insights y recomendaciones para reducir el impacto ambiental, fomentando prácticas de IA más sostenibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lmh_OntsR9uh",
    "outputId": "c905c06b-4e6f-4626-829c-9706aabc3fc7"
   },
   "outputs": [],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    history = model.fit(xs, ys, epochs=70, batch_size=128, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar datos de consumo energético del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:3333/\n",
      "\n",
      " * Serving Flask app 'codecarbon.viz.carbonboard'\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:3333\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/deps/polyfill@7.v2_12_1m1692783890.12.1.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/deps/react@16.v2_12_1m1692783890.14.0.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash_bootstrap_components/_components/dash_bootstrap_components.v1_5_0m1705348573.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/deps/react-dom@16.v2_12_1m1692783890.14.0.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/dash-renderer/build/dash_renderer.v2_12_1m1692783890.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/dcc/dash_core_components.v2_11_1m1692783890.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/deps/prop-types@15.v2_12_1m1692783890.8.1.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/dcc/dash_core_components-shared.v2_11_1m1692783890.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/html/dash_html_components.v2_0_14m1692783890.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:49] \"GET /_dash-component-suites/dash/dash_table/bundle.v5_2_7m1692783890.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_favicon.ico?v=2.12.1 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"GET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:50] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:51] \"GET /assets/house_icon.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:51] \"GET /assets/car_icon.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:51] \"GET /assets/tv_icon.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:51] \"GET /_favicon.ico?v=2.12.1 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Jan/2024 12:34:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "!carbonboard --filepath=\"emissions.csv\" --port=3333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de eficacia del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CvgUbHbPjww"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "XT4XmpYMPoH4",
    "outputId": "fde69c6d-91ff-4088-bfa6-2efd00d58bd2"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de un texto nuevo\n",
    "* Le damos unas palabras para que pueda crear unas lineas contextualizadas en el Quijote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "OimcHXq0Po7Z",
    "outputId": "bf2e77b7-5838-4f77-fb0d-e3fb6353e089"
   },
   "outputs": [],
   "source": [
    "seed_text = \"leer libros de caballerías\"\n",
    "next_words = 50\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se guarda la predicción en un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usm3bZLWMFrx"
   },
   "outputs": [],
   "source": [
    "file = open(\"prediction.txt\", \"w\") \n",
    "file.write(seed_text) \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YYtokmLIg2Zz",
    "outputId": "fbf47681-6607-4cfd-bde7-4d26bad2b5f5"
   },
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LtJwKrzrcq1Z",
    "outputId": "875cc1ab-58cf-43e5-fe21-8bd59266f13a"
   },
   "outputs": [],
   "source": [
    "#cantidad de palabras unicas\n",
    "len(tokenizer.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0lRlhleT01L"
   },
   "outputs": [],
   "source": [
    "#Convertimos el diccionario en una lista, obteniendo solo las palabras\n",
    "words_list = [(k) for k in tokenizer.word_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "EBvA01deg5tJ",
    "outputId": "b96c99a0-9d3d-4881-842e-343f8955def7"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(words_list):\n",
    "  vec = weights[num+1] # skip 0, it's padding.\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "   pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rh3LrlRlcNV_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "QuijoteNLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
